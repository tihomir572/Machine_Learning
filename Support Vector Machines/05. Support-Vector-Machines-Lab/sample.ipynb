{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8da29999fa548f09465d0b7e18732e9",
     "grade": false,
     "grade_id": "cell-7186dfefece144c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nose.tools import *\n",
    "\n",
    "np.random.seed(24680)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your imports in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9ff523820808ea082177fae7c6bf6c4",
     "grade": false,
     "grade_id": "cell-98c4a3a245e45b8a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,KFold\n",
    "from sklearn.svm import LinearSVC ,SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models and Support Vector Machines Lab\n",
    "## Training and comparing different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll work with the bank dataset. This time, the data preprocessing steps have been done for you.\n",
    "\n",
    "The goal is to try and improve our predictions (if they can be improved at all) using different types of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read the data (1 point)\n",
    "This time you only need to read the data. The indicator variables have been separated out for you.\n",
    "\n",
    "Read the dataset and save it in the variable `bank_data`. The target column is `y`. Use the variables `bank_attributes` and `bank_labels` to save the attributes (explanatory variables, features, predictors), and labels (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5332a28d728ce835574c4a823df35a67",
     "grade": false,
     "grade_id": "cell-990a0dfca3a695e6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv(\"data/bank.csv\")\n",
    "bank_attributes = bank_data.drop(columns = [\"y\"])\n",
    "bank_labels = bank_data[\"y\"]\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbb74ffbafaa9dbd29f5ecf74c4ea33d",
     "grade": true,
     "grade_id": "cell-dccbbb44b14b188f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(bank_data)\n",
    "assert_is_not_none(bank_attributes)\n",
    "assert_is_not_none(bank_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalize the data (1 point)\n",
    "Because both forests and SVMs are sensitive to non-scaled data, we need to normalize our dataset first.\n",
    "\n",
    "Rescale all columns in `bank_attributes` so they have mean = 0 and variance = 1. You can either look at the `sklearn` docs or do this yourself. When you're ready, overwrite the `bank_attributes` column. Make sure that you don't lose the column names in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c353a4a1516901f4222f00fc29885cfb",
     "grade": false,
     "grade_id": "cell-7cc2e9c90f8705c5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "column_names = bank_attributes.columns\n",
    "bank_attributes = MinMaxScaler().fit_transform(bank_attributes)\n",
    "bank_attributes = pd.DataFrame(bank_attributes)\n",
    "bank_attributes.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee58ea1d1a912cdf0b8e594249301c55",
     "grade": true,
     "grade_id": "cell-918a6111ee668331",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(bank_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the data (1 point)\n",
    "Use the standard 70% / 30% split. Since this is a classification problem, be sure to stratify the split according to the `bank_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2a9c6e60a741863517ff47df38195db",
     "grade": false,
     "grade_id": "cell-68893dc2052dd87c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "bank_attributes_train,bank_attributes_test, bank_labels_train,bank_labels_test = train_test_split(bank_attributes,bank_labels,stratify = bank_labels,test_size = 0.3,shuffle = True)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3164, 51), (1357, 51), (3164,), (1357,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bank_attributes_train.shape,bank_attributes_test.shape, bank_labels_train.shape,bank_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abc82412e81ffd8b54d9eba9a4f5c943",
     "grade": true,
     "grade_id": "cell-215de695ca1c45ff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(bank_attributes_train)\n",
    "assert_is_not_none(bank_labels_train)\n",
    "\n",
    "assert_is_not_none(bank_attributes_test)\n",
    "assert_is_not_none(bank_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the cross-validation folds (1 point)\n",
    "Use a stratified k-fold cross-validation split, with $k = 5$. Fit it to the train data. Save the trained cross-validator to the variable `k_fold`.\n",
    "\n",
    "The data should already be shuffled. There's no need to shuffle it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9f2a480f45546e6abaf83a538698a7f",
     "grade": false,
     "grade_id": "cell-ef1a3912d43342f1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits = 5)\n",
    "# GridSearchCV(estimator=svm.SVC(max_iter=1000),param_grid={\n",
    "#     'C':[0.01,0.1,1,10,100],\n",
    "#     'loss':['hinge','squared_hinge']\n",
    "# },cv=k_fold)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51421f786ff3b725b11aacb56e7643c8",
     "grade": true,
     "grade_id": "cell-8c28c6c421c7705a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision Tree (2 points)\n",
    "Use cross-validation to train and optimize the hyperparameters for a decision tree classifier.\n",
    "\n",
    "Use grid search with the following grid:\n",
    "* `max_depth`: 1, 5, 7, 15, 20\n",
    "* `min_samples_leaf`: 2, 5, 10, 12\n",
    "* `max_leaf_nodes`: 5, 10, 20\n",
    "\n",
    "Use the most appropriate scoring metric (remember that accuracy doesn't work in this case because the data is highly imbalanced; we need something which combines precision and recall). Use the cross-validation splits you just created.\n",
    "\n",
    "Save the grid results in `grid_search`. Save the best classifier in `tree_classifier`.\n",
    "\n",
    "Optionally, you can print and / or visualize the cross-validation results and the best chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "937595c41be7032fb2dc438a72db8a70",
     "grade": false,
     "grade_id": "cell-3603341dc3a0e1b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree; best score: 0.48698726126425945\n"
     ]
    }
   ],
   "source": [
    "grid_search =GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = {\n",
    "    \"max_depth\": [1, 5, 7, 15, 20],\n",
    "    'min_samples_leaf': [2, 5, 10, 12],\n",
    "    'max_leaf_nodes': [5, 10, 20]\n",
    "}, \n",
    "                          cv = k_fold,scoring=make_scorer(f1_score))\n",
    "grid_search.fit(bank_attributes_train,bank_labels_train)\n",
    "tree_classifier =grid_search.best_estimator_\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(\"Decision tree; best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=7, max_leaf_nodes=10, min_samples_leaf=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=7, max_leaf_nodes=10, min_samples_leaf=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=7, max_leaf_nodes=10, min_samples_leaf=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0a46b7e34d4f16e0c62f00ecf18393a",
     "grade": true,
     "grade_id": "cell-8a9d43f3f99d1c09",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(grid_search)\n",
    "assert_is_not_none(tree_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest (1 point)\n",
    "Use cross-validation to train and optimize the hyperparameters for a random forest classifier. Use the same technique as before.\n",
    "\n",
    "Use the following grid:\n",
    "* `n_estimators`: 100, 200, 300 \n",
    "* `max_depth`: 20, 50, 100\n",
    "\n",
    "Note that this grid is on the small side but this is mainly due to performance reasons. Also note that the training will take some time.\n",
    "\n",
    "Save the grid results in `grid_search`. Save the best classifier in `forest_classifier`.\n",
    "\n",
    "Optionally, you can print and / or visualize the cross-validation results and the best chosen parameters.\n",
    "\n",
    "Due to the relatively slow training, we've chosen low values for the parameters. The performance of the random forest will be worse than the decision tree. This is not necessarily the case in general, it's due to the parameters we've chosen to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "671769b328ea72590acfd5212698421c",
     "grade": false,
     "grade_id": "cell-965717809a95920b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest; best score: 0.3865169599140891\n"
     ]
    }
   ],
   "source": [
    "grid_search =GridSearchCV(estimator = RandomForestClassifier(), param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [20, 50, 100]\n",
    "}, \n",
    "                          cv = k_fold,scoring=make_scorer(f1_score))\n",
    "grid_search.fit(bank_attributes_train,bank_labels_train)\n",
    "forest_classifier = grid_search.best_estimator_\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Random forest; best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14e77aede1b15c71b9e6802012516c1d",
     "grade": true,
     "grade_id": "cell-5cdc884e24e42fec",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(grid_search)\n",
    "assert_is_not_none(forest_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Linear SVM (1 point)\n",
    "Use cross-validation to train and optimize the hyperparameters for a linear support vector machine. Use the same technique as before.\n",
    "\n",
    "Use the following grid:\n",
    "* `C`: 0.1, 0.5, 0.8, 1, 1.5, 2, 6, 10, 15, 20\n",
    "\n",
    "Note that we're choosing relatively small values for `C`. This is allowed because our data is normalized.\n",
    "\n",
    "Save the grid results in `grid_search`. Save the best classifier in `linear_svm_classifier`. There are many ways to create a linear SVM classifier. Look up the `sklearn` docs to choose the fastest one (in terms of performance).\n",
    "\n",
    "Optionally, you can print and / or visualize the cross-validation results and the best chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e19571c5e83d0b7d29d953b9ace63b42",
     "grade": false,
     "grade_id": "cell-dbc74911ea58e898",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM; best score: 0.3964890331597575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tihomir111\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_search =GridSearchCV(estimator = LinearSVC(), param_grid = {\n",
    "    'C': [ 0.1, 0.5, 0.8, 1, 1.5, 2, 6, 10, 15, 20]\n",
    "}, \n",
    "                          cv = k_fold,scoring=make_scorer(f1_score))\n",
    "grid_search.fit(bank_attributes_train,bank_labels_train)\n",
    "linear_svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(\"Linear SVM; best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "375ffa7de497664fe4089895b7dca13b",
     "grade": true,
     "grade_id": "cell-7e5b16fe3d5deb03",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(grid_search)\n",
    "assert_is_not_none(linear_svm_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Gaussian SVM (1 point)\n",
    "Use cross-validation to train and optimize the hyperparameters for an SVM with a Gaussian kernel. Use the same technique as before.\n",
    "\n",
    "Use the following grid:\n",
    "* `C`: 10, 15, 20, 50, 200\n",
    "* `gamma`: 0.001, 0.01, 0.1, 0.2\n",
    "\n",
    "Note that this time we give larger values of `C` because the governing parameter here is `gamma`.\n",
    "\n",
    "Save the grid results in `grid_search`. Save the best classifier in `gaussian_svm_classifier`.\n",
    "\n",
    "Optionally, you can print and / or visualize the cross-validation results and the best chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e33ae1cd0f54a05b532e9df39d850a1a",
     "grade": false,
     "grade_id": "cell-06ab026bf98b2e69",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian SVM; best score: 0.4290787528000507\n"
     ]
    }
   ],
   "source": [
    "grid_search =GridSearchCV(estimator = SVC(), param_grid = {\n",
    "    'C': [10, 15, 20, 50, 200],\n",
    "    'gamma': [0.001, 0.01, 0.1, 0.2]\n",
    "}, \n",
    "                          cv = k_fold,scoring=make_scorer(f1_score))\n",
    "grid_search.fit(bank_attributes_train,bank_labels_train)\n",
    "gaussian_svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(\"Gaussian SVM; best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c81b3a26de11990ff354e812c694a31",
     "grade": true,
     "grade_id": "cell-0ba2a8ae7d45a6ad",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(grid_search)\n",
    "assert_is_not_none(gaussian_svm_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Compare performance on the testing data (1 point)\n",
    "Now that you've trained all your models, you've got to select the best one. This should be done on the testing data.\n",
    "\n",
    "Use the appropriate scoring metric to get the testing scores for all your models. Don't forget to pass the **testing**, not the training data. Save all scores.\n",
    "\n",
    "Choose the best classifier, based on these scores (the one with the highest test score). Of course, this is not enough. We need to look at ROC curves, track performance through other measures, debug the sources of variance in testing results, try more hyperparameters, etc. However, this is enough for an introductory lab :).\n",
    "\n",
    "Optionally, you can think of combining them into a boosted model but this is out of the scope of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e04bd8cb12cc90f38bff21bb552e4f3d",
     "grade": false,
     "grade_id": "cell-cf416134ae2b2a62",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scores:\n",
      "Decision tree: 0.4638783269961977\n",
      "Random forest: 0.3348837209302326\n",
      "Linear SVM: 0.4052863436123348\n",
      "Gaussian SVM: 0.4107744107744108\n"
     ]
    }
   ],
   "source": [
    "tree_classifier_score, forest_classifier_score, linear_svm_classifier_score, gaussian_svm_classifier_score = (f1_score(bank_labels_test, tree_classifier.predict(bank_attributes_test)), \n",
    "f1_score(bank_labels_test, forest_classifier.predict(bank_attributes_test)), \n",
    "f1_score(bank_labels_test, linear_svm_classifier.predict(bank_attributes_test)), \n",
    "f1_score(bank_labels_test, gaussian_svm_classifier.predict(bank_attributes_test)), \n",
    ")\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Testing scores:\")\n",
    "print(\"Decision tree:\", tree_classifier_score)\n",
    "print(\"Random forest:\", forest_classifier_score)\n",
    "print(\"Linear SVM:\", linear_svm_classifier_score)\n",
    "print(\"Gaussian SVM:\", gaussian_svm_classifier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4454d84b6f6e3b4ab3d4cadf37b2e1c0",
     "grade": false,
     "grade_id": "cell-bdeed4c0d761fe0e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "best_classifier = \"Linear SVM\" # Replace empty string with \"tree\", \"forest\", \"linear SVM\" or \"gaussian SVM\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b497b75ac0a01e38900e2d05760f637",
     "grade": true,
     "grade_id": "cell-495fc5ef0619deb6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_not_equal(best_classifier, \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
